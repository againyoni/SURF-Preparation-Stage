{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Image Detection",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Object Detection in Images\n",
        "There are three types of deep learning on images: classification, detection and segmentation.\n",
        "\n",
        "From classification, the images could be classified in the last report.\n",
        "In this report, image detection will be implemented. For instance, in a give picture, the result will detect if an object is a human, animal or plant.\n",
        "\n",
        "Deep learning algorithms for image classification and segmentation are:\n",
        "AlexNet, GoogleNet, MobileNet and VGGNet\n",
        "\n",
        "On the other hand, object detection is based on sailent features, mostly using SSD-MobileNetv2, SSD-MobileNetv3 or YOLO v1 and v2."
      ],
      "metadata": {
        "id": "T_SejtZpxNaL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "2E5sHcyAxMHH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# the configPath and weightsPath are both downloaded from the website:\n",
        "# https://github.com/opencv/opencv/wiki/TensorFlow-Object-Detection-API\n",
        "configPath = 'ssd_mobilenet_v3_large_coco_2020_01_14.pbtxt'\n",
        "weightsPath = 'frozen_inference_graph.pb'\n",
        "\n",
        "cvNet = cv2.dnn_DetectionModel(weightsPath, configPath)\n",
        "\n",
        "img = cv2.imread('example.jpg')\n",
        "plt.imshow(img)\n",
        "\"\"\"\n",
        "# these functions are used for direct detection from webcam\n",
        "cap = cv.VideoCapture(1) \n",
        "cap.set(3, 648)\n",
        "cap.set(4, 488)\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 355
        },
        "id": "rIEpLBp0EIK7",
        "outputId": "78b29077-b7e0-42dc-d3d2-736624a386df"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SystemError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
            "\u001b[0;31merror\u001b[0m: OpenCV(4.1.2) /io/opencv/modules/dnn/src/caffe/caffe_io.cpp:1132: error: (-2:Unspecified error) FAILED: fs.is_open(). Can't open \"frozen_inference_graph.pb\" in function 'ReadProtoFromBinaryFile'\n",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mSystemError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-09d39ec2922f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mweightsPath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'frozen_inference_graph.pb'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mcvNet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdnn_DetectionModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweightsPath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfigPath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'example.jpg'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mSystemError\u001b[0m: <class 'cv2.dnn_DetectionModel'> returned a result with an error set"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import the image that will be used to detect objects. The image is named as example.jpg, and read with imread() method.\n",
        "\n",
        "cv.dnn is the mostly used method to load the model for image detection. The model is provided from the OpenCV official website.\n",
        "\n",
        "We then need labels to check the right answers is obtained."
      ],
      "metadata": {
        "id": "XaVYj6hgJW0k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "classNames = []\n",
        "# classes for the object detection are copied from the following website:\n",
        "# https://github.com/pjreddie/darknet/blob/master/data/coco.names\n",
        "classFile = 'coco.names.txt'\n",
        "with open(classFile, 'rt') as f:\n",
        "    classNames = f.read().rstrip('\\n').split('\\n') # this is the same as classNames.append(f.read())\n",
        "    \n",
        "print(classNames)\n",
        "\n",
        "# these are set by default when using dnn_DetectionModel\n",
        "cvNet.setInputSize(320, 320)\n",
        "cvNet.setInputScale(1.0/127.5) # 255/2 = 127.5\n",
        "cvNet.setInputMean((127.5, 127.5, 127.5)) # Mobilenet => [-1, 1]\n",
        "cvNet.setInputSwapRB(True)\n",
        "\n",
        "# while True:\n",
        "# success, img = cap.read()\n",
        "classIds, confs, bbox = cvNet.detect(img, confThreshold = 0.5)\n",
        "\n",
        "# set the font size and type\n",
        "font_scale = 1\n",
        "font = cv2.FONT_HERSHEY_PLAIN\n",
        "\n",
        "# inside the while True loop, if len(classIds != 0):\n",
        "for classId, confidence, box in zip(classIds.flatten(), confs.flatten(), bbox):\n",
        "    cv2.rectangle(img, box, color = (0, 255, 0), thickness = 3) # setting the rectangles to the detected objects\n",
        "    cv2.putText(img, classNames[classId - 1], (box[0] + 10, box[1] + 30),\n",
        "               font, fontScale = font_scale, color = (0, 255, 0),thickness = 2)"
      ],
      "metadata": {
        "id": "-nWHxQrfGCAV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        },
        "outputId": "e59453be-43bd-4a30-eccd-9ca2725f1915"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-6acc554327e7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# https://github.com/pjreddie/darknet/blob/master/data/coco.names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mclassFile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'coco.names.txt'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclassFile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rt'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mclassNames\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# this is the same as classNames.append(f.read())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'coco.names.txt'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "className is in an array to store each line of the classes.\n",
        "The txt file that 80 classes are stored is imported, then saved in the array.\n",
        "\n",
        "With the dnn_DetectionModel default values, the input scales for the picture is setted up."
      ],
      "metadata": {
        "id": "UFyJrLuGpFZW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# show image with rectangles marked\n",
        "cv2.imshow('img', img)\n",
        "cv2.waitKey()"
      ],
      "metadata": {
        "id": "Z842iYh8I6Ht"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It shows the original image with rectangle that detects objects. As an instance with the example.jpg picture, it will give two rectangles labeled with dog and cat."
      ],
      "metadata": {
        "id": "cR57Dbn5uZ2V"
      }
    }
  ]
}