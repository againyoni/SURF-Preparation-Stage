{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Numerical Operations\nimport math\nimport numpy as np\n\n# Reading/Writing Data\nimport pandas as pd\nimport os\nimport csv\n\n# For Progress Bar\nfrom tqdm import tqdm\n\n# Pytorch\nimport torch \nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader, random_split\n\n# For plotting learning curve\nfrom torch.utils.tensorboard import SummaryWriter\n\nfrom sklearn.model_selection import KFold","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-11-02T05:07:00.845297Z","iopub.execute_input":"2022-11-02T05:07:00.845708Z","iopub.status.idle":"2022-11-02T05:07:00.854239Z","shell.execute_reply.started":"2022-11-02T05:07:00.845676Z","shell.execute_reply":"2022-11-02T05:07:00.852714Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"device = 'cuda' if torch.cuda.is_available() else 'cpu'\nkfold_n = 5","metadata":{"execution":{"iopub.status.busy":"2022-11-02T05:07:00.856519Z","iopub.execute_input":"2022-11-02T05:07:00.857055Z","iopub.status.idle":"2022-11-02T05:07:00.869568Z","shell.execute_reply.started":"2022-11-02T05:07:00.857018Z","shell.execute_reply":"2022-11-02T05:07:00.868384Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"def same_seed(seed): \n    '''Fixes random number generator seeds for reproducibility.'''\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)","metadata":{"execution":{"iopub.status.busy":"2022-11-02T05:07:00.871010Z","iopub.execute_input":"2022-11-02T05:07:00.871378Z","iopub.status.idle":"2022-11-02T05:07:00.879734Z","shell.execute_reply.started":"2022-11-02T05:07:00.871347Z","shell.execute_reply":"2022-11-02T05:07:00.878558Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"def kFold(data_set):\n    train_k = []\n    valid_k = []\n    \n    KF = KFold(n_splits = kfold_n)\n    for train_index, valid_index in KF.split(data_set):\n        train_set, valid_set = data_set[train_index], data_set[valid_index]\n        train_k.append(np.array(train_set))\n        valid_k.append(np.array(valid_set))\n    \n    return np.array(train_k), np.array(valid_k)","metadata":{"execution":{"iopub.status.busy":"2022-11-02T05:07:00.882634Z","iopub.execute_input":"2022-11-02T05:07:00.883337Z","iopub.status.idle":"2022-11-02T05:07:00.892622Z","shell.execute_reply.started":"2022-11-02T05:07:00.883289Z","shell.execute_reply":"2022-11-02T05:07:00.891505Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"class CustomDataset(Dataset):\n    def __init__(self, x, y=None):\n        if y is None:\n            self.y_data = y\n        else:\n            self.y_data = torch.FloatTensor(y)\n        self.x_data = torch.FloatTensor(x)\n\n    def __getitem__(self, idx):\n        if self.y_data is None:\n            return self.x_data[idx]\n        else:\n            return self.x_data[idx], self.y_data[idx]\n\n    def __len__(self):\n        return len(self.x_data)","metadata":{"execution":{"iopub.status.busy":"2022-11-02T05:07:00.894394Z","iopub.execute_input":"2022-11-02T05:07:00.894949Z","iopub.status.idle":"2022-11-02T05:07:00.905827Z","shell.execute_reply.started":"2022-11-02T05:07:00.894903Z","shell.execute_reply":"2022-11-02T05:07:00.904612Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"class My_Model(nn.Module):\n    def __init__(self, input_dim):\n        super(My_Model, self).__init__()\n        # making the model more elaborative helps reducing the loss\n        # trial 1: set Linear(), input_dim -> 16 -> 8 -> 1\n        # result: could generate the result, but the loss is not reduced satisfactorilly\n        \n        # trial 2: 128, and reduce by /2\n        # result: at some folds, the train loss could not be decreased,\n        #         as well as the validation loss.\n        #         they are overfitted, so reduce the to 64\n        \n        # trial 3: 64 -> 16 -> then reduce by 2\n        # result: it produces the most loss at around 1.5 or less.\n        \n        self.layers = nn.Sequential(\n            nn.Linear(input_dim, 64),\n            nn.ReLU(),\n            nn.Linear(64, 16),\n            nn.ReLU(),\n            nn.Linear(16, 8),\n            nn.ReLU(),\n            nn.Linear(8, 4),\n            nn.ReLU(),\n            nn.Linear(4, 2),\n            nn.ReLU(),\n            nn.Linear(2, 1)\n        )\n\n    def forward(self, x):\n        x = self.layers(x)\n        x = x.squeeze(1)\n        return x","metadata":{"execution":{"iopub.status.busy":"2022-11-02T05:07:00.907738Z","iopub.execute_input":"2022-11-02T05:07:00.908112Z","iopub.status.idle":"2022-11-02T05:07:00.919475Z","shell.execute_reply.started":"2022-11-02T05:07:00.908056Z","shell.execute_reply":"2022-11-02T05:07:00.917923Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"def select_feat(train_data, valid_data, test_data, select_all=True):\n    '''Selects useful features to perform regression'''\n    y_train, y_valid = train_data[:,-1], valid_data[:,-1]\n    raw_x_train, raw_x_valid, raw_x_test = train_data[:,:-1], valid_data[:,:-1], test_data\n\n    if select_all:\n        feat_idx = list(range(raw_x_train.shape[1]))\n    else:\n        feat_idx = [0,1,2,3,4] # TODO: Select suitable feature columns.\n        \n    return raw_x_train[:,feat_idx], raw_x_valid[:,feat_idx], raw_x_test[:,feat_idx], y_train, y_valid","metadata":{"execution":{"iopub.status.busy":"2022-11-02T05:07:00.921285Z","iopub.execute_input":"2022-11-02T05:07:00.922599Z","iopub.status.idle":"2022-11-02T05:07:00.937812Z","shell.execute_reply.started":"2022-11-02T05:07:00.922538Z","shell.execute_reply":"2022-11-02T05:07:00.936524Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"def trainer(train_loader, valid_loader, model, k, device):\n\n    criterion = nn.MSELoss(reduction='mean') # Define your loss function, do not modify this.\n    optimizer = torch.optim.SGD(model.parameters(), lr=1e-5, momentum=0.9) \n\n    writer = SummaryWriter() # Writer of tensoboard.\n\n    if not os.path.isdir('./models'):\n        os.mkdir('./models') # Create directory of saving models.\n\n    n_epochs, best_loss, step, early_stop_count = 3000, math.inf, 0, 0\n\n    for epoch in range(n_epochs):\n        model.train() # Set your model to train mode.\n        loss_record = []\n\n        # tqdm is a package to visualize your training progress.\n        train_pbar = tqdm(train_loader, position=0, leave=True)\n\n        for x, y in train_pbar:\n            optimizer.zero_grad()               # Set gradient to zero.\n            x, y = x.to(device), y.to(device)   # Move your data to device. \n            pred = model(x)             \n            loss = criterion(pred, y)\n            loss.backward()                     # Compute gradient(backpropagation).\n            optimizer.step()                    # Update parameters.\n            step += 1\n            loss_record.append(loss.detach().item())\n            \n            # Display current epoch number and loss on tqdm progress bar.\n            train_pbar.set_description(f'Epoch [{epoch+1}/{n_epochs}]')\n            train_pbar.set_postfix({'loss': loss.detach().item()})\n\n        mean_train_loss = sum(loss_record)/len(loss_record)\n        writer.add_scalar('Loss/train', mean_train_loss, step)\n\n        model.eval() # Set your model to evaluation mode.\n        loss_record = []\n        for x, y in valid_loader:\n            x, y = x.to(device), y.to(device)\n            with torch.no_grad():\n                pred = model(x)\n                loss = criterion(pred, y)\n\n            loss_record.append(loss.item())\n            \n        mean_valid_loss = sum(loss_record)/len(loss_record)\n        print(f'Epoch [{epoch+1}/{n_epochs}]: Train loss: {mean_train_loss:.4f}, Valid loss: {mean_valid_loss:.4f}')\n        writer.add_scalar('Loss/valid', mean_valid_loss, step)\n\n        if mean_valid_loss < best_loss:\n            best_loss = mean_valid_loss\n            torch.save(model.state_dict(), f'./models/model{k}.ckpt') # Save your best model\n            print('Saving model with loss {:.3f}...'.format(best_loss))\n            early_stop_count = 0\n        else: \n            early_stop_count += 1\n\n        if early_stop_count >= 400:\n            print('\\nModel is not improving, so we halt the training session.')\n            return","metadata":{"execution":{"iopub.status.busy":"2022-11-02T05:07:00.941909Z","iopub.execute_input":"2022-11-02T05:07:00.942463Z","iopub.status.idle":"2022-11-02T05:07:00.964913Z","shell.execute_reply.started":"2022-11-02T05:07:00.942399Z","shell.execute_reply":"2022-11-02T05:07:00.963463Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"code","source":"# Set seed for reproducibility\nsame_seed(5201314)\n\ntrain_data, test_data = pd.read_csv('../input/ml2022spring-hw1/covid.train.csv').values, pd.read_csv('../input/ml2022spring-hw1/covid.test.csv').values\ntrain_data, valid_data = kFold(train_data)\n\nfor k in range(kfold_n):\n    # Select features\n    x_train, x_valid, x_test, y_train, y_valid = select_feat(train_data[k], valid_data[k], test_data, True)\n\n    train_dataset, valid_dataset, test_dataset = CustomDataset(x_train, y_train), \\\n                                                CustomDataset(x_valid, y_valid), \\\n                                                CustomDataset(x_test)\n\n    # Pytorch data loader loads pytorch dataset into batches.\n    train_loader = DataLoader(train_dataset, batch_size=256, shuffle=True, pin_memory=True)\n    valid_loader = DataLoader(valid_dataset, batch_size=256, shuffle=True, pin_memory=True)\n    test_loader = DataLoader(test_dataset, batch_size=256, shuffle=False, pin_memory=True)\n    \n    print('************************************')\n    print(f'{k+1} Fold:')\n    print('************************************')\n    \n    model = My_Model(input_dim=x_train.shape[1]).to(device) # put your model and data on the same computation device.\n    trainer(train_loader, valid_loader, model, k, device)","metadata":{"execution":{"iopub.status.busy":"2022-11-02T05:07:00.966659Z","iopub.execute_input":"2022-11-02T05:07:00.968264Z","iopub.status.idle":"2022-11-02T05:07:14.545015Z","shell.execute_reply.started":"2022-11-02T05:07:00.968196Z","shell.execute_reply":"2022-11-02T05:07:14.543207Z"},"trusted":true},"execution_count":38,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:11: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n  # This is added back by InteractiveShellApp.init_path()\n","output_type":"stream"},{"name":"stdout","text":"************************************\n1 Fold:\n************************************\n","output_type":"stream"},{"name":"stderr","text":"Epoch [1/3000]: 100%|██████████| 9/9 [00:00<00:00, 152.88it/s, loss=111]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [1/3000]: Train loss: 141.5305, Valid loss: 90.2108\nSaving model with loss 90.211...\n","output_type":"stream"},{"name":"stderr","text":"Epoch [2/3000]: 100%|██████████| 9/9 [00:00<00:00, 154.75it/s, loss=67.9]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [2/3000]: Train loss: 101.5903, Valid loss: 73.5882\nSaving model with loss 73.588...\n","output_type":"stream"},{"name":"stderr","text":"Epoch [3/3000]: 100%|██████████| 9/9 [00:00<00:00, 151.62it/s, loss=52.8]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [3/3000]: Train loss: 57.4230, Valid loss: 62.6156\nSaving model with loss 62.616...\n","output_type":"stream"},{"name":"stderr","text":"Epoch [4/3000]: 100%|██████████| 9/9 [00:00<00:00, 148.85it/s, loss=43.5]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [4/3000]: Train loss: 50.1467, Valid loss: 79.9454\n","output_type":"stream"},{"name":"stderr","text":"Epoch [5/3000]: 100%|██████████| 9/9 [00:00<00:00, 143.39it/s, loss=48.9]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [5/3000]: Train loss: 49.0189, Valid loss: 69.0409\n","output_type":"stream"},{"name":"stderr","text":"Epoch [6/3000]: 100%|██████████| 9/9 [00:00<00:00, 151.79it/s, loss=41.6]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [6/3000]: Train loss: 47.5916, Valid loss: 77.0545\n","output_type":"stream"},{"name":"stderr","text":"Epoch [7/3000]: 100%|██████████| 9/9 [00:00<00:00, 150.58it/s, loss=46.9]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [7/3000]: Train loss: 46.7998, Valid loss: 67.9822\n","output_type":"stream"},{"name":"stderr","text":"Epoch [8/3000]: 100%|██████████| 9/9 [00:00<00:00, 156.19it/s, loss=42.8]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [8/3000]: Train loss: 46.3555, Valid loss: 66.7080\n","output_type":"stream"},{"name":"stderr","text":"Epoch [9/3000]: 100%|██████████| 9/9 [00:00<00:00, 143.16it/s, loss=53.1]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [9/3000]: Train loss: 46.7445, Valid loss: 68.5353\n","output_type":"stream"},{"name":"stderr","text":"Epoch [10/3000]: 100%|██████████| 9/9 [00:00<00:00, 139.05it/s, loss=47.2]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [10/3000]: Train loss: 46.3235, Valid loss: 62.3344\nSaving model with loss 62.334...\n","output_type":"stream"},{"name":"stderr","text":"Epoch [11/3000]: 100%|██████████| 9/9 [00:00<00:00, 160.39it/s, loss=45.7]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [11/3000]: Train loss: 46.0433, Valid loss: 59.1754\nSaving model with loss 59.175...\n","output_type":"stream"},{"name":"stderr","text":"Epoch [12/3000]: 100%|██████████| 9/9 [00:00<00:00, 163.85it/s, loss=39.4]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [12/3000]: Train loss: 45.5717, Valid loss: 69.9897\n","output_type":"stream"},{"name":"stderr","text":"Epoch [13/3000]: 100%|██████████| 9/9 [00:00<00:00, 150.15it/s, loss=45.7]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [13/3000]: Train loss: 45.8614, Valid loss: 60.2751\n","output_type":"stream"},{"name":"stderr","text":"Epoch [14/3000]: 100%|██████████| 9/9 [00:00<00:00, 139.25it/s, loss=47]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [14/3000]: Train loss: 45.6868, Valid loss: 61.5784\n","output_type":"stream"},{"name":"stderr","text":"Epoch [15/3000]: 100%|██████████| 9/9 [00:00<00:00, 149.42it/s, loss=50.3]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [15/3000]: Train loss: 45.8956, Valid loss: 68.4353\n","output_type":"stream"},{"name":"stderr","text":"Epoch [16/3000]: 100%|██████████| 9/9 [00:00<00:00, 154.77it/s, loss=49.4]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [16/3000]: Train loss: 45.6110, Valid loss: 59.0975\nSaving model with loss 59.098...\n","output_type":"stream"},{"name":"stderr","text":"Epoch [17/3000]: 100%|██████████| 9/9 [00:00<00:00, 167.42it/s, loss=41.8]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [17/3000]: Train loss: 45.0257, Valid loss: 63.5397\n","output_type":"stream"},{"name":"stderr","text":"Epoch [18/3000]: 100%|██████████| 9/9 [00:00<00:00, 171.34it/s, loss=47.7]","output_type":"stream"},{"name":"stdout","text":"Epoch [18/3000]: Train loss: 45.3075, Valid loss: 56.5872\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Saving model with loss 56.587...\n","output_type":"stream"},{"name":"stderr","text":"Epoch [19/3000]: 100%|██████████| 9/9 [00:00<00:00, 156.37it/s, loss=38.4]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [19/3000]: Train loss: 44.5568, Valid loss: 54.2388\nSaving model with loss 54.239...\n","output_type":"stream"},{"name":"stderr","text":"Epoch [20/3000]: 100%|██████████| 9/9 [00:00<00:00, 163.74it/s, loss=46]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [20/3000]: Train loss: 45.0506, Valid loss: 63.1402\n","output_type":"stream"},{"name":"stderr","text":"Epoch [21/3000]: 100%|██████████| 9/9 [00:00<00:00, 134.46it/s, loss=34.7]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [21/3000]: Train loss: 44.2008, Valid loss: 52.5735\nSaving model with loss 52.573...\n","output_type":"stream"},{"name":"stderr","text":"Epoch [22/3000]: 100%|██████████| 9/9 [00:00<00:00, 128.78it/s, loss=42.6]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [22/3000]: Train loss: 44.6045, Valid loss: 62.4627\n","output_type":"stream"},{"name":"stderr","text":"Epoch [23/3000]: 100%|██████████| 9/9 [00:00<00:00, 130.73it/s, loss=49.3]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [23/3000]: Train loss: 44.7178, Valid loss: 55.3816\n","output_type":"stream"},{"name":"stderr","text":"Epoch [24/3000]: 100%|██████████| 9/9 [00:00<00:00, 127.65it/s, loss=39.9]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [24/3000]: Train loss: 43.9608, Valid loss: 59.1045\n","output_type":"stream"},{"name":"stderr","text":"Epoch [25/3000]: 100%|██████████| 9/9 [00:00<00:00, 127.88it/s, loss=35.3]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [25/3000]: Train loss: 43.5389, Valid loss: 60.0237\n","output_type":"stream"},{"name":"stderr","text":"Epoch [26/3000]: 100%|██████████| 9/9 [00:00<00:00, 161.11it/s, loss=38.1]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [26/3000]: Train loss: 43.4577, Valid loss: 56.9088\n","output_type":"stream"},{"name":"stderr","text":"Epoch [27/3000]: 100%|██████████| 9/9 [00:00<00:00, 162.56it/s, loss=37.7]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [27/3000]: Train loss: 43.2087, Valid loss: 58.1603\n","output_type":"stream"},{"name":"stderr","text":"Epoch [28/3000]: 100%|██████████| 9/9 [00:00<00:00, 137.07it/s, loss=46.7]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [28/3000]: Train loss: 43.5592, Valid loss: 56.5074\n","output_type":"stream"},{"name":"stderr","text":"Epoch [29/3000]: 100%|██████████| 9/9 [00:00<00:00, 121.52it/s, loss=36.2]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [29/3000]: Train loss: 42.7559, Valid loss: 58.5701\n","output_type":"stream"},{"name":"stderr","text":"Epoch [30/3000]: 100%|██████████| 9/9 [00:00<00:00, 124.91it/s, loss=54.6]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [30/3000]: Train loss: 43.6865, Valid loss: 55.4819\n","output_type":"stream"},{"name":"stderr","text":"Epoch [31/3000]: 100%|██████████| 9/9 [00:00<00:00, 126.38it/s, loss=41.9]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [31/3000]: Train loss: 42.5644, Valid loss: 53.1751\n","output_type":"stream"},{"name":"stderr","text":"Epoch [32/3000]: 100%|██████████| 9/9 [00:00<00:00, 154.34it/s, loss=40.4]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [32/3000]: Train loss: 42.0807, Valid loss: 50.0304\nSaving model with loss 50.030...\n","output_type":"stream"},{"name":"stderr","text":"Epoch [33/3000]: 100%|██████████| 9/9 [00:00<00:00, 151.92it/s, loss=46.8]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [33/3000]: Train loss: 42.1311, Valid loss: 43.6711\nSaving model with loss 43.671...\n","output_type":"stream"},{"name":"stderr","text":"Epoch [34/3000]: 100%|██████████| 9/9 [00:00<00:00, 128.67it/s, loss=39.2]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [34/3000]: Train loss: 41.3228, Valid loss: 46.2717\n","output_type":"stream"},{"name":"stderr","text":"Epoch [35/3000]: 100%|██████████| 9/9 [00:00<00:00, 138.09it/s, loss=48.2]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [35/3000]: Train loss: 41.7217, Valid loss: 39.9704\nSaving model with loss 39.970...\n","output_type":"stream"},{"name":"stderr","text":"Epoch [36/3000]: 100%|██████████| 9/9 [00:00<00:00, 136.73it/s, loss=45.3]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [36/3000]: Train loss: 41.1260, Valid loss: 45.4940\n","output_type":"stream"},{"name":"stderr","text":"Epoch [37/3000]: 100%|██████████| 9/9 [00:00<00:00, 157.78it/s, loss=39.4]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [37/3000]: Train loss: 40.1370, Valid loss: 43.6212\n","output_type":"stream"},{"name":"stderr","text":"Epoch [38/3000]: 100%|██████████| 9/9 [00:00<00:00, 122.79it/s, loss=38.6]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [38/3000]: Train loss: 39.5401, Valid loss: 38.1171\nSaving model with loss 38.117...\n","output_type":"stream"},{"name":"stderr","text":"Epoch [39/3000]: 100%|██████████| 9/9 [00:00<00:00, 113.14it/s, loss=37.5]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [39/3000]: Train loss: 38.6688, Valid loss: 29.6782\nSaving model with loss 29.678...\n","output_type":"stream"},{"name":"stderr","text":"Epoch [40/3000]: 100%|██████████| 9/9 [00:00<00:00, 128.82it/s, loss=39.7]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [40/3000]: Train loss: 38.4043, Valid loss: 38.4173\n","output_type":"stream"},{"name":"stderr","text":"Epoch [41/3000]: 100%|██████████| 9/9 [00:00<00:00, 105.73it/s, loss=33]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [41/3000]: Train loss: 36.8340, Valid loss: 30.5304\n","output_type":"stream"},{"name":"stderr","text":"Epoch [42/3000]: 100%|██████████| 9/9 [00:00<00:00, 118.90it/s, loss=29.3]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [42/3000]: Train loss: 35.5245, Valid loss: 29.3057\nSaving model with loss 29.306...\n","output_type":"stream"},{"name":"stderr","text":"Epoch [43/3000]: 100%|██████████| 9/9 [00:00<00:00, 108.34it/s, loss=35.7]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [43/3000]: Train loss: 35.0375, Valid loss: 29.1247\nSaving model with loss 29.125...\n","output_type":"stream"},{"name":"stderr","text":"Epoch [44/3000]: 100%|██████████| 9/9 [00:00<00:00, 148.58it/s, loss=31.9]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [44/3000]: Train loss: 33.7163, Valid loss: 22.9810\nSaving model with loss 22.981...\n","output_type":"stream"},{"name":"stderr","text":"Epoch [45/3000]: 100%|██████████| 9/9 [00:00<00:00, 153.94it/s, loss=33.7]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [45/3000]: Train loss: 32.5066, Valid loss: 19.0603\nSaving model with loss 19.060...\n","output_type":"stream"},{"name":"stderr","text":"Epoch [46/3000]: 100%|██████████| 9/9 [00:00<00:00, 155.73it/s, loss=34.9]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [46/3000]: Train loss: 30.9808, Valid loss: 17.8611\nSaving model with loss 17.861...\n","output_type":"stream"},{"name":"stderr","text":"Epoch [47/3000]: 100%|██████████| 9/9 [00:00<00:00, 151.80it/s, loss=24.1]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [47/3000]: Train loss: 29.4048, Valid loss: 14.0112\nSaving model with loss 14.011...\n","output_type":"stream"},{"name":"stderr","text":"Epoch [48/3000]: 100%|██████████| 9/9 [00:00<00:00, 149.63it/s, loss=25]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [48/3000]: Train loss: 28.0681, Valid loss: 20.3244\n","output_type":"stream"},{"name":"stderr","text":"Epoch [49/3000]: 100%|██████████| 9/9 [00:00<00:00, 152.52it/s, loss=30.3]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [49/3000]: Train loss: 26.1521, Valid loss: 19.8903\n","output_type":"stream"},{"name":"stderr","text":"Epoch [50/3000]: 100%|██████████| 9/9 [00:00<00:00, 153.37it/s, loss=30.7]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [50/3000]: Train loss: 24.7149, Valid loss: 17.2505\n","output_type":"stream"},{"name":"stderr","text":"Epoch [51/3000]: 100%|██████████| 9/9 [00:00<00:00, 151.56it/s, loss=21.7]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [51/3000]: Train loss: 22.7397, Valid loss: 18.4203\n","output_type":"stream"},{"name":"stderr","text":"Epoch [52/3000]: 100%|██████████| 9/9 [00:00<00:00, 152.20it/s, loss=21.6]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [52/3000]: Train loss: 22.5549, Valid loss: 18.6629\n","output_type":"stream"},{"name":"stderr","text":"Epoch [53/3000]: 100%|██████████| 9/9 [00:00<00:00, 155.06it/s, loss=21.9]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [53/3000]: Train loss: 19.6777, Valid loss: 15.6980\n","output_type":"stream"},{"name":"stderr","text":"Epoch [54/3000]: 100%|██████████| 9/9 [00:00<00:00, 146.66it/s, loss=16.6]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [54/3000]: Train loss: 15.9889, Valid loss: 14.7522\n","output_type":"stream"},{"name":"stderr","text":"Epoch [55/3000]: 100%|██████████| 9/9 [00:00<00:00, 140.07it/s, loss=11.7]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [55/3000]: Train loss: 13.3049, Valid loss: 14.6178\n","output_type":"stream"},{"name":"stderr","text":"Epoch [56/3000]: 100%|██████████| 9/9 [00:00<00:00, 161.44it/s, loss=7.52]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [56/3000]: Train loss: 11.4813, Valid loss: 12.3295\nSaving model with loss 12.329...\n","output_type":"stream"},{"name":"stderr","text":"Epoch [57/3000]: 100%|██████████| 9/9 [00:00<00:00, 164.32it/s, loss=9.22]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [57/3000]: Train loss: 13.3556, Valid loss: 11.6439\nSaving model with loss 11.644...\n","output_type":"stream"},{"name":"stderr","text":"Epoch [58/3000]: 100%|██████████| 9/9 [00:00<00:00, 156.52it/s, loss=11.3]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [58/3000]: Train loss: 12.2225, Valid loss: 9.5435\nSaving model with loss 9.543...\n","output_type":"stream"},{"name":"stderr","text":"Epoch [59/3000]: 100%|██████████| 9/9 [00:00<00:00, 165.35it/s, loss=38.1]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [59/3000]: Train loss: 17.6795, Valid loss: 10.6981\n","output_type":"stream"},{"name":"stderr","text":"Epoch [60/3000]: 100%|██████████| 9/9 [00:00<00:00, 143.15it/s, loss=55]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [60/3000]: Train loss: 45.2403, Valid loss: 18.5078\n","output_type":"stream"},{"name":"stderr","text":"Epoch [61/3000]: 100%|██████████| 9/9 [00:00<00:00, 145.56it/s, loss=32.8]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [61/3000]: Train loss: 39.8655, Valid loss: 28.1714\n","output_type":"stream"},{"name":"stderr","text":"Epoch [62/3000]: 100%|██████████| 9/9 [00:00<00:00, 144.01it/s, loss=31.8]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [62/3000]: Train loss: 35.1739, Valid loss: 16.1904\n","output_type":"stream"},{"name":"stderr","text":"Epoch [63/3000]: 100%|██████████| 9/9 [00:00<00:00, 135.17it/s, loss=26.2]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [63/3000]: Train loss: 26.2608, Valid loss: 15.3759\n","output_type":"stream"},{"name":"stderr","text":"Epoch [64/3000]: 100%|██████████| 9/9 [00:00<00:00, 160.72it/s, loss=18]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [64/3000]: Train loss: 20.6720, Valid loss: 15.3478\n","output_type":"stream"},{"name":"stderr","text":"Epoch [65/3000]: 100%|██████████| 9/9 [00:00<00:00, 162.67it/s, loss=15.1]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [65/3000]: Train loss: 15.6526, Valid loss: 19.0242\n","output_type":"stream"},{"name":"stderr","text":"Epoch [66/3000]: 100%|██████████| 9/9 [00:00<00:00, 151.79it/s, loss=11]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [66/3000]: Train loss: 11.8015, Valid loss: 33.2161\n","output_type":"stream"},{"name":"stderr","text":"Epoch [67/3000]: 100%|██████████| 9/9 [00:00<00:00, 159.98it/s, loss=8.76]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [67/3000]: Train loss: 9.2964, Valid loss: 40.8425\n","output_type":"stream"},{"name":"stderr","text":"Epoch [68/3000]: 100%|██████████| 9/9 [00:00<00:00, 133.47it/s, loss=10.1]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [68/3000]: Train loss: 8.6781, Valid loss: 42.6014\n","output_type":"stream"},{"name":"stderr","text":"Epoch [69/3000]: 100%|██████████| 9/9 [00:00<00:00, 159.80it/s, loss=8.67]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [69/3000]: Train loss: 8.4028, Valid loss: 29.9316\n","output_type":"stream"},{"name":"stderr","text":"Epoch [70/3000]: 100%|██████████| 9/9 [00:00<00:00, 158.90it/s, loss=6.68]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [70/3000]: Train loss: 6.9513, Valid loss: 23.4935\n","output_type":"stream"},{"name":"stderr","text":"Epoch [71/3000]: 100%|██████████| 9/9 [00:00<00:00, 157.06it/s, loss=5.77]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [71/3000]: Train loss: 6.5957, Valid loss: 31.2282\n","output_type":"stream"},{"name":"stderr","text":"Epoch [72/3000]: 100%|██████████| 9/9 [00:00<00:00, 138.16it/s, loss=13]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [72/3000]: Train loss: 9.0383, Valid loss: 19.1703\n","output_type":"stream"},{"name":"stderr","text":"Epoch [73/3000]: 100%|██████████| 9/9 [00:00<00:00, 169.98it/s, loss=14.4]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [73/3000]: Train loss: 32.5713, Valid loss: 24.1663\n","output_type":"stream"},{"name":"stderr","text":"Epoch [74/3000]: 100%|██████████| 9/9 [00:00<00:00, 128.15it/s, loss=23.6]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [74/3000]: Train loss: 32.0949, Valid loss: 27.6768\n","output_type":"stream"},{"name":"stderr","text":"Epoch [75/3000]: 100%|██████████| 9/9 [00:00<00:00, 157.75it/s, loss=15.4]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [75/3000]: Train loss: 18.1665, Valid loss: 24.3084\n","output_type":"stream"},{"name":"stderr","text":"Epoch [76/3000]: 100%|██████████| 9/9 [00:00<00:00, 165.64it/s, loss=9.76]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [76/3000]: Train loss: 13.2576, Valid loss: 29.3430\n","output_type":"stream"},{"name":"stderr","text":"Epoch [77/3000]: 100%|██████████| 9/9 [00:00<00:00, 164.32it/s, loss=8.43]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [77/3000]: Train loss: 10.4407, Valid loss: 28.1697\n","output_type":"stream"},{"name":"stderr","text":"Epoch [78/3000]: 100%|██████████| 9/9 [00:00<00:00, 161.69it/s, loss=8.41]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [78/3000]: Train loss: 8.1294, Valid loss: 25.6334\n","output_type":"stream"},{"name":"stderr","text":"Epoch [79/3000]: 100%|██████████| 9/9 [00:00<00:00, 148.85it/s, loss=9.01]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [79/3000]: Train loss: 21.0925, Valid loss: 18.8973\n","output_type":"stream"},{"name":"stderr","text":"Epoch [80/3000]: 100%|██████████| 9/9 [00:00<00:00, 160.15it/s, loss=12.2]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [80/3000]: Train loss: 17.0365, Valid loss: 16.0381\n","output_type":"stream"},{"name":"stderr","text":"Epoch [81/3000]: 100%|██████████| 9/9 [00:00<00:00, 157.23it/s, loss=10.8]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [81/3000]: Train loss: 13.4197, Valid loss: 21.6076\n","output_type":"stream"},{"name":"stderr","text":"Epoch [82/3000]: 100%|██████████| 9/9 [00:00<00:00, 171.54it/s, loss=8.45]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [82/3000]: Train loss: 10.3202, Valid loss: 29.0021\n","output_type":"stream"},{"name":"stderr","text":"Epoch [83/3000]: 100%|██████████| 9/9 [00:00<00:00, 161.86it/s, loss=7.07]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [83/3000]: Train loss: 7.5136, Valid loss: 30.2107\n","output_type":"stream"},{"name":"stderr","text":"Epoch [84/3000]: 100%|██████████| 9/9 [00:00<00:00, 131.58it/s, loss=6.79]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [84/3000]: Train loss: 6.7821, Valid loss: 26.8061\n","output_type":"stream"},{"name":"stderr","text":"Epoch [85/3000]: 100%|██████████| 9/9 [00:00<00:00, 129.92it/s, loss=6.94]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [85/3000]: Train loss: 6.6316, Valid loss: 22.6570\n","output_type":"stream"},{"name":"stderr","text":"Epoch [86/3000]: 100%|██████████| 9/9 [00:00<00:00, 143.87it/s, loss=4.77]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [86/3000]: Train loss: 6.7734, Valid loss: 20.8203\n","output_type":"stream"},{"name":"stderr","text":"Epoch [87/3000]: 100%|██████████| 9/9 [00:00<00:00, 141.84it/s, loss=7.73]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [87/3000]: Train loss: 6.2976, Valid loss: 23.5647\n","output_type":"stream"},{"name":"stderr","text":"Epoch [88/3000]: 100%|██████████| 9/9 [00:00<00:00, 136.87it/s, loss=5.64]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [88/3000]: Train loss: 6.1416, Valid loss: 22.6107\n","output_type":"stream"},{"name":"stderr","text":"Epoch [89/3000]: 100%|██████████| 9/9 [00:00<00:00, 139.83it/s, loss=6.7]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [89/3000]: Train loss: 6.1132, Valid loss: 20.7542\n","output_type":"stream"},{"name":"stderr","text":"Epoch [90/3000]: 100%|██████████| 9/9 [00:00<00:00, 148.28it/s, loss=7.09]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [90/3000]: Train loss: 6.3856, Valid loss: 22.9633\n","output_type":"stream"},{"name":"stderr","text":"Epoch [91/3000]: 100%|██████████| 9/9 [00:00<00:00, 163.87it/s, loss=6.2]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [91/3000]: Train loss: 7.6345, Valid loss: 19.3250\n","output_type":"stream"},{"name":"stderr","text":"Epoch [92/3000]: 100%|██████████| 9/9 [00:00<00:00, 160.65it/s, loss=5.17]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [92/3000]: Train loss: 6.2443, Valid loss: 22.3730\n","output_type":"stream"},{"name":"stderr","text":"Epoch [93/3000]: 100%|██████████| 9/9 [00:00<00:00, 147.78it/s, loss=9.13]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [93/3000]: Train loss: 6.8976, Valid loss: 22.5965\n","output_type":"stream"},{"name":"stderr","text":"Epoch [94/3000]: 100%|██████████| 9/9 [00:00<00:00, 142.91it/s, loss=5.61]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [94/3000]: Train loss: 6.1678, Valid loss: 21.8279\n","output_type":"stream"},{"name":"stderr","text":"Epoch [95/3000]: 100%|██████████| 9/9 [00:00<00:00, 136.58it/s, loss=7.19]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [95/3000]: Train loss: 6.3534, Valid loss: 22.6518\n","output_type":"stream"},{"name":"stderr","text":"Epoch [96/3000]: 100%|██████████| 9/9 [00:00<00:00, 146.62it/s, loss=6.82]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [96/3000]: Train loss: 7.3206, Valid loss: 20.7100\n","output_type":"stream"},{"name":"stderr","text":"Epoch [97/3000]: 100%|██████████| 9/9 [00:00<00:00, 155.85it/s, loss=5.76]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [97/3000]: Train loss: 6.2918, Valid loss: 21.9073\n","output_type":"stream"},{"name":"stderr","text":"Epoch [98/3000]: 100%|██████████| 9/9 [00:00<00:00, 162.08it/s, loss=5.73]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [98/3000]: Train loss: 6.3788, Valid loss: 24.4196\n","output_type":"stream"},{"name":"stderr","text":"Epoch [99/3000]: 100%|██████████| 9/9 [00:00<00:00, 163.68it/s, loss=5.77]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [99/3000]: Train loss: 6.0155, Valid loss: 23.6115\n","output_type":"stream"},{"name":"stderr","text":"Epoch [100/3000]: 100%|██████████| 9/9 [00:00<00:00, 160.19it/s, loss=6.49]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [100/3000]: Train loss: 7.5620, Valid loss: 24.8072\n","output_type":"stream"},{"name":"stderr","text":"Epoch [101/3000]: 100%|██████████| 9/9 [00:00<00:00, 156.07it/s, loss=6.77]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [101/3000]: Train loss: 10.8445, Valid loss: 18.3395\n","output_type":"stream"},{"name":"stderr","text":"Epoch [102/3000]: 100%|██████████| 9/9 [00:00<00:00, 152.72it/s, loss=5.7]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [102/3000]: Train loss: 7.1555, Valid loss: 22.5582\n","output_type":"stream"},{"name":"stderr","text":"Epoch [103/3000]: 100%|██████████| 9/9 [00:00<00:00, 161.39it/s, loss=5.94]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [103/3000]: Train loss: 6.6057, Valid loss: 23.4598\n","output_type":"stream"},{"name":"stderr","text":"Epoch [104/3000]: 100%|██████████| 9/9 [00:00<00:00, 135.25it/s, loss=6.45]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [104/3000]: Train loss: 5.6881, Valid loss: 20.6644\n","output_type":"stream"},{"name":"stderr","text":"Epoch [105/3000]: 100%|██████████| 9/9 [00:00<00:00, 125.99it/s, loss=5.24]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [105/3000]: Train loss: 7.8204, Valid loss: 20.6689\n","output_type":"stream"},{"name":"stderr","text":"Epoch [106/3000]: 100%|██████████| 9/9 [00:00<00:00, 161.58it/s, loss=4.85]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [106/3000]: Train loss: 6.3017, Valid loss: 20.1807\n","output_type":"stream"},{"name":"stderr","text":"Epoch [107/3000]: 100%|██████████| 9/9 [00:00<00:00, 141.11it/s, loss=5.85]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [107/3000]: Train loss: 7.1204, Valid loss: 15.6230\n","output_type":"stream"},{"name":"stderr","text":"Epoch [108/3000]: 100%|██████████| 9/9 [00:00<00:00, 140.06it/s, loss=7.69]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [108/3000]: Train loss: 6.3238, Valid loss: 21.2504\n","output_type":"stream"},{"name":"stderr","text":"Epoch [109/3000]: 100%|██████████| 9/9 [00:00<00:00, 136.84it/s, loss=5.5]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [109/3000]: Train loss: 6.4903, Valid loss: 20.7025\n","output_type":"stream"},{"name":"stderr","text":"Epoch [110/3000]: 100%|██████████| 9/9 [00:00<00:00, 34.83it/s, loss=6.2] \n","output_type":"stream"},{"name":"stdout","text":"Epoch [110/3000]: Train loss: 6.2955, Valid loss: 21.6872\n","output_type":"stream"},{"name":"stderr","text":"Epoch [111/3000]: 100%|██████████| 9/9 [00:00<00:00, 138.72it/s, loss=5.75]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [111/3000]: Train loss: 5.7241, Valid loss: 18.4031\n","output_type":"stream"},{"name":"stderr","text":"Epoch [112/3000]: 100%|██████████| 9/9 [00:00<00:00, 163.36it/s, loss=4.94]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [112/3000]: Train loss: 6.0742, Valid loss: 21.5108\n","output_type":"stream"},{"name":"stderr","text":"Epoch [113/3000]: 100%|██████████| 9/9 [00:00<00:00, 152.48it/s, loss=6.83]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [113/3000]: Train loss: 6.0689, Valid loss: 19.6917\n","output_type":"stream"},{"name":"stderr","text":"Epoch [114/3000]: 100%|██████████| 9/9 [00:00<00:00, 157.09it/s, loss=13.9]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [114/3000]: Train loss: 8.8899, Valid loss: 18.8455\n","output_type":"stream"},{"name":"stderr","text":"Epoch [115/3000]: 100%|██████████| 9/9 [00:00<00:00, 145.37it/s, loss=5.49]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [115/3000]: Train loss: 7.3128, Valid loss: 18.3086\n","output_type":"stream"},{"name":"stderr","text":"Epoch [116/3000]: 100%|██████████| 9/9 [00:00<00:00, 156.67it/s, loss=6.25]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [116/3000]: Train loss: 5.8964, Valid loss: 20.3150\n","output_type":"stream"},{"name":"stderr","text":"Epoch [117/3000]: 100%|██████████| 9/9 [00:00<00:00, 162.93it/s, loss=5.4]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [117/3000]: Train loss: 5.7688, Valid loss: 21.4279\n","output_type":"stream"},{"name":"stderr","text":"Epoch [118/3000]: 100%|██████████| 9/9 [00:00<00:00, 155.79it/s, loss=6.91]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [118/3000]: Train loss: 5.6185, Valid loss: 23.8538\n","output_type":"stream"},{"name":"stderr","text":"Epoch [119/3000]: 100%|██████████| 9/9 [00:00<00:00, 154.79it/s, loss=4.39]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [119/3000]: Train loss: 6.0172, Valid loss: 16.6658\n","output_type":"stream"},{"name":"stderr","text":"Epoch [120/3000]: 100%|██████████| 9/9 [00:00<00:00, 147.43it/s, loss=7.52]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [120/3000]: Train loss: 6.9684, Valid loss: 19.4248\n","output_type":"stream"},{"name":"stderr","text":"Epoch [121/3000]: 100%|██████████| 9/9 [00:00<00:00, 130.67it/s, loss=4.7]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [121/3000]: Train loss: 6.1177, Valid loss: 19.8798\n","output_type":"stream"},{"name":"stderr","text":"Epoch [122/3000]: 100%|██████████| 9/9 [00:00<00:00, 150.13it/s, loss=5.84]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [122/3000]: Train loss: 6.0908, Valid loss: 19.8285\n","output_type":"stream"},{"name":"stderr","text":"Epoch [123/3000]: 100%|██████████| 9/9 [00:00<00:00, 143.54it/s, loss=5.26]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [123/3000]: Train loss: 5.3111, Valid loss: 18.4691\n","output_type":"stream"},{"name":"stderr","text":"Epoch [124/3000]: 100%|██████████| 9/9 [00:00<00:00, 164.54it/s, loss=6.02]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [124/3000]: Train loss: 5.3851, Valid loss: 20.9739\n","output_type":"stream"},{"name":"stderr","text":"Epoch [125/3000]: 100%|██████████| 9/9 [00:00<00:00, 144.56it/s, loss=5.92]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [125/3000]: Train loss: 6.0641, Valid loss: 18.2423\n","output_type":"stream"},{"name":"stderr","text":"Epoch [126/3000]: 100%|██████████| 9/9 [00:00<00:00, 140.35it/s, loss=9.57]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [126/3000]: Train loss: 6.3935, Valid loss: 21.7481\n","output_type":"stream"},{"name":"stderr","text":"Epoch [127/3000]: 100%|██████████| 9/9 [00:00<00:00, 146.26it/s, loss=5.91]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [127/3000]: Train loss: 7.4959, Valid loss: 19.9996\n","output_type":"stream"},{"name":"stderr","text":"Epoch [128/3000]: 100%|██████████| 9/9 [00:00<00:00, 154.16it/s, loss=6.11]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [128/3000]: Train loss: 7.6948, Valid loss: 20.4532\n","output_type":"stream"},{"name":"stderr","text":"Epoch [129/3000]: 100%|██████████| 9/9 [00:00<00:00, 159.15it/s, loss=4.41]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [129/3000]: Train loss: 5.5628, Valid loss: 17.4214\n","output_type":"stream"},{"name":"stderr","text":"Epoch [130/3000]: 100%|██████████| 9/9 [00:00<00:00, 164.38it/s, loss=5.43]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [130/3000]: Train loss: 5.4047, Valid loss: 25.5662\n","output_type":"stream"},{"name":"stderr","text":"Epoch [131/3000]: 100%|██████████| 9/9 [00:00<00:00, 120.76it/s, loss=6.06]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [131/3000]: Train loss: 6.0683, Valid loss: 21.7019\n","output_type":"stream"},{"name":"stderr","text":"Epoch [132/3000]: 100%|██████████| 9/9 [00:00<00:00, 150.12it/s, loss=4.75]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [132/3000]: Train loss: 5.1951, Valid loss: 17.7464\n","output_type":"stream"},{"name":"stderr","text":"Epoch [133/3000]: 100%|██████████| 9/9 [00:00<00:00, 154.96it/s, loss=5.38]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [133/3000]: Train loss: 6.0967, Valid loss: 22.2517\n","output_type":"stream"},{"name":"stderr","text":"Epoch [134/3000]: 100%|██████████| 9/9 [00:00<00:00, 152.57it/s, loss=6.23]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [134/3000]: Train loss: 7.4318, Valid loss: 16.9467\n","output_type":"stream"},{"name":"stderr","text":"Epoch [135/3000]: 100%|██████████| 9/9 [00:00<00:00, 173.07it/s, loss=9.18]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [135/3000]: Train loss: 7.9818, Valid loss: 17.0381\n","output_type":"stream"},{"name":"stderr","text":"Epoch [136/3000]: 100%|██████████| 9/9 [00:00<00:00, 171.19it/s, loss=7.37]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [136/3000]: Train loss: 6.2856, Valid loss: 23.4187\n","output_type":"stream"},{"name":"stderr","text":"Epoch [137/3000]: 100%|██████████| 9/9 [00:00<00:00, 169.40it/s, loss=3.49]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [137/3000]: Train loss: 5.3079, Valid loss: 19.6893\n","output_type":"stream"},{"name":"stderr","text":"Epoch [138/3000]: 100%|██████████| 9/9 [00:00<00:00, 169.60it/s, loss=4.15]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [138/3000]: Train loss: 5.4232, Valid loss: 17.6507\n","output_type":"stream"},{"name":"stderr","text":"Epoch [139/3000]: 100%|██████████| 9/9 [00:00<00:00, 153.91it/s, loss=4.31]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [139/3000]: Train loss: 5.4949, Valid loss: 17.9160\n","output_type":"stream"},{"name":"stderr","text":"Epoch [140/3000]: 100%|██████████| 9/9 [00:00<00:00, 156.84it/s, loss=4.43]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [140/3000]: Train loss: 5.4802, Valid loss: 18.9629\n","output_type":"stream"},{"name":"stderr","text":"Epoch [141/3000]: 100%|██████████| 9/9 [00:00<00:00, 153.32it/s, loss=5.8]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [141/3000]: Train loss: 6.1307, Valid loss: 17.6542\n","output_type":"stream"},{"name":"stderr","text":"Epoch [142/3000]: 100%|██████████| 9/9 [00:00<00:00, 157.55it/s, loss=4.89]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [142/3000]: Train loss: 7.1443, Valid loss: 19.8424\n","output_type":"stream"},{"name":"stderr","text":"Epoch [143/3000]: 100%|██████████| 9/9 [00:00<00:00, 164.93it/s, loss=4.37]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [143/3000]: Train loss: 5.5621, Valid loss: 17.2400\n","output_type":"stream"},{"name":"stderr","text":"Epoch [144/3000]: 100%|██████████| 9/9 [00:00<00:00, 170.60it/s, loss=4.96]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [144/3000]: Train loss: 5.3272, Valid loss: 22.1838\n","output_type":"stream"},{"name":"stderr","text":"Epoch [145/3000]: 100%|██████████| 9/9 [00:00<00:00, 151.17it/s, loss=5.11]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [145/3000]: Train loss: 5.2098, Valid loss: 19.2515\n","output_type":"stream"},{"name":"stderr","text":"Epoch [146/3000]: 100%|██████████| 9/9 [00:00<00:00, 96.38it/s, loss=4.5]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [146/3000]: Train loss: 5.3807, Valid loss: 19.2268\n","output_type":"stream"},{"name":"stderr","text":"Epoch [147/3000]: 100%|██████████| 9/9 [00:00<00:00, 153.89it/s, loss=5.63]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [147/3000]: Train loss: 5.1443, Valid loss: 18.4593\n","output_type":"stream"},{"name":"stderr","text":"Epoch [148/3000]: 100%|██████████| 9/9 [00:00<00:00, 167.71it/s, loss=5.23]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [148/3000]: Train loss: 5.0014, Valid loss: 18.5423\n","output_type":"stream"},{"name":"stderr","text":"Epoch [149/3000]: 100%|██████████| 9/9 [00:00<00:00, 166.06it/s, loss=4.87]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [149/3000]: Train loss: 5.2260, Valid loss: 19.3448\n","output_type":"stream"},{"name":"stderr","text":"Epoch [150/3000]: 100%|██████████| 9/9 [00:00<00:00, 160.02it/s, loss=6.26]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [150/3000]: Train loss: 6.7247, Valid loss: 16.8604\n","output_type":"stream"},{"name":"stderr","text":"Epoch [151/3000]: 100%|██████████| 9/9 [00:00<00:00, 161.05it/s, loss=3.84]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [151/3000]: Train loss: 6.7974, Valid loss: 17.8536\n","output_type":"stream"},{"name":"stderr","text":"Epoch [152/3000]: 100%|██████████| 9/9 [00:00<00:00, 161.12it/s, loss=5.15]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [152/3000]: Train loss: 7.2484, Valid loss: 18.4457\n","output_type":"stream"},{"name":"stderr","text":"Epoch [153/3000]: 100%|██████████| 9/9 [00:00<00:00, 154.15it/s, loss=6.89]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [153/3000]: Train loss: 5.8995, Valid loss: 16.3107\n","output_type":"stream"},{"name":"stderr","text":"Epoch [154/3000]: 100%|██████████| 9/9 [00:00<00:00, 158.41it/s, loss=5.81]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [154/3000]: Train loss: 5.4218, Valid loss: 16.7200\n","output_type":"stream"},{"name":"stderr","text":"Epoch [155/3000]: 100%|██████████| 9/9 [00:00<00:00, 162.58it/s, loss=6.93]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [155/3000]: Train loss: 5.7100, Valid loss: 15.6825\n","output_type":"stream"},{"name":"stderr","text":"Epoch [156/3000]: 100%|██████████| 9/9 [00:00<00:00, 150.37it/s, loss=8.86]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [156/3000]: Train loss: 6.6819, Valid loss: 19.1962\n","output_type":"stream"},{"name":"stderr","text":"Epoch [157/3000]: 100%|██████████| 9/9 [00:00<00:00, 144.86it/s, loss=6.52]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [157/3000]: Train loss: 6.4492, Valid loss: 20.1929\n","output_type":"stream"},{"name":"stderr","text":"Epoch [158/3000]: 100%|██████████| 9/9 [00:00<00:00, 151.28it/s, loss=3.98]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [158/3000]: Train loss: 5.1194, Valid loss: 23.8043\n","output_type":"stream"},{"name":"stderr","text":"Epoch [159/3000]: 100%|██████████| 9/9 [00:00<00:00, 149.54it/s, loss=4.08]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [159/3000]: Train loss: 4.8665, Valid loss: 19.1913\n","output_type":"stream"},{"name":"stderr","text":"Epoch [160/3000]: 100%|██████████| 9/9 [00:00<00:00, 141.32it/s, loss=5.9]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [160/3000]: Train loss: 5.1394, Valid loss: 20.7459\n","output_type":"stream"},{"name":"stderr","text":"Epoch [161/3000]: 100%|██████████| 9/9 [00:00<00:00, 131.76it/s, loss=5.57]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [161/3000]: Train loss: 7.0078, Valid loss: 19.8526\n","output_type":"stream"},{"name":"stderr","text":"Epoch [162/3000]: 100%|██████████| 9/9 [00:00<00:00, 145.39it/s, loss=4.24]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [162/3000]: Train loss: 5.0960, Valid loss: 19.3783\n","output_type":"stream"},{"name":"stderr","text":"Epoch [163/3000]: 100%|██████████| 9/9 [00:00<00:00, 142.17it/s, loss=4.59]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [163/3000]: Train loss: 6.9185, Valid loss: 20.1684\n","output_type":"stream"},{"name":"stderr","text":"Epoch [164/3000]: 100%|██████████| 9/9 [00:00<00:00, 164.46it/s, loss=5.16]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [164/3000]: Train loss: 6.8237, Valid loss: 18.9068\n","output_type":"stream"},{"name":"stderr","text":"Epoch [165/3000]: 100%|██████████| 9/9 [00:00<00:00, 162.06it/s, loss=4.27]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [165/3000]: Train loss: 5.5590, Valid loss: 23.0422\n","output_type":"stream"},{"name":"stderr","text":"Epoch [166/3000]: 100%|██████████| 9/9 [00:00<00:00, 158.90it/s, loss=5.97]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [166/3000]: Train loss: 5.1106, Valid loss: 17.0591\n","output_type":"stream"},{"name":"stderr","text":"Epoch [167/3000]: 100%|██████████| 9/9 [00:00<00:00, 130.59it/s, loss=6.03]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [167/3000]: Train loss: 6.5214, Valid loss: 19.4991\n","output_type":"stream"},{"name":"stderr","text":"Epoch [168/3000]: 100%|██████████| 9/9 [00:00<00:00, 141.22it/s, loss=3.91]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [168/3000]: Train loss: 5.7110, Valid loss: 17.1510\n","output_type":"stream"},{"name":"stderr","text":"Epoch [169/3000]: 100%|██████████| 9/9 [00:00<00:00, 135.91it/s, loss=4.74]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [169/3000]: Train loss: 5.7864, Valid loss: 18.2059\n","output_type":"stream"},{"name":"stderr","text":"Epoch [170/3000]: 100%|██████████| 9/9 [00:00<00:00, 133.00it/s, loss=4.28]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [170/3000]: Train loss: 5.5342, Valid loss: 18.3716\n","output_type":"stream"},{"name":"stderr","text":"Epoch [171/3000]: 100%|██████████| 9/9 [00:00<00:00, 138.33it/s, loss=6.24]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [171/3000]: Train loss: 5.5234, Valid loss: 22.0138\n","output_type":"stream"},{"name":"stderr","text":"Epoch [172/3000]: 100%|██████████| 9/9 [00:00<00:00, 136.43it/s, loss=5.76]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [172/3000]: Train loss: 5.7872, Valid loss: 16.3206\n","output_type":"stream"},{"name":"stderr","text":"Epoch [173/3000]: 100%|██████████| 9/9 [00:00<00:00, 150.78it/s, loss=5.97]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [173/3000]: Train loss: 5.2160, Valid loss: 18.9384\n","output_type":"stream"},{"name":"stderr","text":"Epoch [174/3000]: 100%|██████████| 9/9 [00:00<00:00, 140.43it/s, loss=7.17]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [174/3000]: Train loss: 5.7736, Valid loss: 18.3392\n","output_type":"stream"},{"name":"stderr","text":"Epoch [175/3000]: 100%|██████████| 9/9 [00:00<00:00, 142.69it/s, loss=4.95]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [175/3000]: Train loss: 5.1911, Valid loss: 20.5985\n","output_type":"stream"},{"name":"stderr","text":"Epoch [176/3000]: 100%|██████████| 9/9 [00:00<00:00, 142.39it/s, loss=5.67]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [176/3000]: Train loss: 4.8909, Valid loss: 19.3678\n","output_type":"stream"},{"name":"stderr","text":"Epoch [177/3000]: 100%|██████████| 9/9 [00:00<00:00, 124.10it/s, loss=3.7]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [177/3000]: Train loss: 4.6697, Valid loss: 18.2343\n","output_type":"stream"},{"name":"stderr","text":"Epoch [178/3000]: 100%|██████████| 9/9 [00:00<00:00, 132.66it/s, loss=5.47]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [178/3000]: Train loss: 4.7629, Valid loss: 18.2129\n","output_type":"stream"},{"name":"stderr","text":"Epoch [179/3000]: 100%|██████████| 9/9 [00:00<00:00, 138.93it/s, loss=5.82]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [179/3000]: Train loss: 4.6061, Valid loss: 17.0193\n","output_type":"stream"},{"name":"stderr","text":"Epoch [180/3000]:  11%|█         | 1/9 [00:00<00:00, 87.87it/s, loss=4.14]\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_27/3095483653.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMy_Model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# put your model and data on the same computation device.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m     \u001b[0mtrainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/tmp/ipykernel_27/2753109400.py\u001b[0m in \u001b[0;36mtrainer\u001b[0;34m(train_loader, valid_loader, model, k, device)\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m                     \u001b[0;31m# Compute gradient(backpropagation).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m                    \u001b[0;31m# Update parameters.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0mstep\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    361\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 363\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    173\u001b[0m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[1;32m    174\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m def grad(\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"code","source":"def predict(test_loader, model, device):\n    model.eval() # Set your model to evaluation mode.\n    preds = []\n    for x in tqdm(test_loader):\n        x = x.to(device)                        \n        with torch.no_grad():                   \n            pred = model(x)                     \n            preds.append(pred.detach().cpu())   \n    preds = torch.cat(preds, dim=0).numpy()  \n    return preds","metadata":{"execution":{"iopub.status.busy":"2022-11-02T05:07:14.546303Z","iopub.status.idle":"2022-11-02T05:07:14.547053Z","shell.execute_reply.started":"2022-11-02T05:07:14.546832Z","shell.execute_reply":"2022-11-02T05:07:14.546856Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def save_pred(preds, file):\n    with open(file, 'w') as fp:\n        writer = csv.writer(fp)\n        writer.writerow(['id', 'tested_positive'])\n        for i, p in enumerate(preds):\n            writer.writerow([i, p])\n\n\ntest_dataset = CustomDataset(test_data, test=True)\ntest_loader = DataLoader(test_dataset, batch_size=256, shuffle=False)\n\npreds = [0] * (train_dataset.__len__())\nfor i in range(kfold_n):\n    model = My_Model(input_dim=train_dataset.__len__()).to(device)\n    model.load_state_dict(torch.load(f'./models/model{i}.ckpt'))\n    for j, element in enumerate(predict(test_loader, model, device).tolist()):\n        preds[j] += element\n\nfor i in range(train_dataset.__len__()):\n    # divide by the number of kfold iteration to get average\n    preds[i] = preds[i]/kfold_n\n    \nsave_pred(preds, 'pred.csv')","metadata":{"execution":{"iopub.status.busy":"2022-11-02T05:07:14.549075Z","iopub.status.idle":"2022-11-02T05:07:14.550058Z","shell.execute_reply.started":"2022-11-02T05:07:14.549832Z","shell.execute_reply":"2022-11-02T05:07:14.549857Z"},"trusted":true},"execution_count":null,"outputs":[]}]}